---
title: Информационный поиск
---

![GoxICKFglOd94az](https://i.loli.net/2019/11/25/GoxICKFglOd94az.jpg)

![2PB8cXlnMGDNLof](https://i.loli.net/2019/11/25/2PB8cXlnMGDNLof.jpg)

**Ранжирование** — это сортировка каких-либо объектов из соображения их относительной релевантности («важности»).

Где применяется:

* Поиск по текстовым документам

* Поиск по другим объектам: картинкам, видео, музыке, товарам, etc.

* Рекомендательные и рейтинговые системы, коллаборативная фильтрация

* Нахождение определений, вопросно-ответные системы

* Оценка важности писем, фильтрация спама

* Выбор лучшего варианта в машинном переводе и распознавании речи

* Автодополнение в умных клавиатурах, подсказки в поиске

* Поиск подходящего ответа в разговорных чат-ботах

**Как измерять качество ранжирования.**

У ранжируемых объектов есть какое-то число $r_i$, соответствующее «эталонной» релевантности. Существует два основных способа его получения:

1. На основе исторических данных. Например, в случае рекомендаций контента, можно взять просмотры (клики, лайки, покупки) пользователя и присвоить просмотренным весам соответствующих элементов 1, а всем остальным — 0.  
2. На основе экспертной оценки. Например, в задаче поиска, для каждого запроса можно привлечь команду асессоров, которые вручную оценят релевантности документов запросу. Обычно оценка категориальная (то есть не действительное число, а несколько градаций, скажем, 2, 3 или 5). 

Дальше возникает задача оценить качество ранжирования в целом, уже имея информацию о релевантности каждого элемента по отдельности. Условно, от идеального ранжирования мы хотим, чтобы объекты шли в порядке убывания релевантности, но есть несколько способов задать такие метрики.

**Precision at k (p@k)** — «точность на $K$ элементах». Допустим, наш алгоритм ранжирования выдал оценки релевантности для каждого элемента. Отобрав среди них первые $k$ элементов с наибольшим $r_i$ посчитаем среди них долю релевантных — это и называется precision at k.

**MRR** — mean reciprocal rank, дословно «средний обратный ранг».

Эта метрика никак не учитывает абсолютные значения релевантности, а только их относительный порядок. Пусть $i$-тый самый релевантный элемент в нашем упорядочивании оказался на позиции $rank_i \in [1, N]$. Тогда MRR считается как:

$$
MRR@k = \frac{1}{K} \sum_{i=1}^K \frac{1}{rank_i}
$$

Чаще всего считают MRR@1, то есть просто по большому количеству запросов смотрят, на какой позиции находится самый подходящий документ, и усредняют обратные к этим позициям.

**DCG** — discounted cumulative gain. Учитывает одновременно и значения релевантностей, и их порядок, путем домножения релевантности на вес, равный обратному логарифму номера позиции:

$$
DCG = \sum_{i=1}^N \frac{rel_i}{\log_2 (i+1)}
$$

У неё есть проблема, связанная с тем, что, по-первых, оценки релевантности могут быть неотмасштабированными, и, во-вторых, на больших последовательностях она выше.

Поэтому вводят $nDCG$ («нормализованный DCG» принимающий значения от 0 до 1), равный отношению DCG данного ранжирования к «идеальному», то есть максимально возможному (iDCG):

$$
nDCG = \frac{DCG}{iDCG}
$$

$iDCG$ можно получить, отсортировав выдачу по реальным релевантностям и посчитав DCG на нём.

**Информационным поиском** (*information retrieval*) называют процесс поиска неструктурированной информации в (большой) коллекции документов, а также некоторые оффтопные задачи, вроде кластеризации документов (склейки дубликатов), нахождения и фильтрации нежелательного контента, аннотирования, и т. д.

Поиск это вообще не очень формально поставленная задача. В сервисах, где на его качество не пофиг, обычно даже разделяют команду на две: одна определяет, что такое хорошая выдача (то есть придумывает метрику), а вторая играется в каггл.

Метрику обычно даже придумывают не сразу, а делают несколько аппроксимаций того, чего в конечном итоге хотят.

* На верхнем уровне может быть что-то вроде количества пользователей (retention, деньги, цена акций etc.).

* Чтобы её оптимизировать, пытаются посчитать условное «счастье пользователя» — по логам целой сессии определить её успешность.

* Потом привлекают асессоров и с учетом оценок пытаются понять, как данное ранжирование результатов повлияло на счастье.

* Потом из этой метрики делают уже что-то, что можно оптимизировать напрямую методами машинного обучения. 

Для поиска есть даже отдельная мини-конференция, на которой занимаются только метриками. Но в конечном итоге последняя, самая интересная часть, сводится к ранжированию.

**pFound.** В Яндексе для поисковых задач когда-то придумали и [до сих пор повсеместно используют](https://catboost.ai/docs/references/pfound.html) метрику, которая называется pfound — грубо говоря, вероятность того, что пользователь найдёт на странице выдачи (среди первых $n$ документов) то, что искал.

$$
pfound = \sum_{i=1}^n pLook(i) \cdot pRel(i)
$$

Где $pLook$ определяется рекурсивно, как вероятность того, что пользователь вообще посмотрит на $i$-тый документ:

$$
pLook(i) = \begin{cases}
1, &i=1\\
pLook(i-1) \cdot (1 - pRel(i-1)), &i > 1
\end{cases}
$$

Интуиция такая: пользователь просматривает страницы выдачи сверху вниз, пока не найдёт релевантный документ, вероятность чего равна $pRel(i) = f(r_i)$, то есть как-то зависит от асессорской оценки релевантности.

### Фичи в поиске

Поисковики часто обвиняют в редактировании результатов, но на самом деле условный Google ещё лет 10 назад потерял понимание того, как работает поиск — потому что всё ранжирование начали делать машинным обучением.

Это работает так: обучается модель, которая учится по паре запрос-документ выдавать одно действительное число — релевантность — по которой и сортируются результаты.

В качестве модели может быть что угодно — нейросеть, линейные модели, решающие деревья на регрессию. Они принимают обычные фичи, какие-то из которых относятся только к запросу, какие-то — только к документу, какие-то — и к запросу, и к документу. Например:

* Легко вычислимые текстовые фичи. Например, сколько слов из запроса встречаются в заголовке, в подзаголовках, во всем тексте.

* Разреженный tf-idf вектор слов из запроса.

* Разная статистика про документ: CTR по запросу, посещаемость сайта, PageRank, сколько в среднем пользователи проводят на этой странице, кликают ли они дальше на какие-то результаты, etc.

* Информация о пользователе: возраст, пол, страна, увлечения, любимые сайты, etc.

* Машинно-обучаемые фичи — например, можно обучить нейросеть, предсказываюзую релевантность по тексту запроса и тексту заголовка.

Часто поиск бывает устроен иерархически — это связано с тем, что считать некоторые фичи и применять поверх них тяжелые модели долго и дорого. Этапов может быть 4-5, например — на первом рассматриваются десятки тысяч документов, а на последнем, например, упорядочивается только несколько десятков — то, что увидит пользователь.

### Индексирование

![](https://miro.medium.com/max/1200/1*Z-lWQkqbDq85SMz6kye5Kw.png)

Какие страницы прокачивать (ресурсы нерезиновые), как часто и в каком формате хранить — отдельные задачи. После того, как данные получены, по ним строится какая-то структура (называемая **индексом**), позволяющая быстро какими-то жадными методами получить по запросу небольшое (не миллиарды, а несколько тысяч) количество документов, которые потом ранжируются чем-то нормальным.

Для текстовых документов обычно строят **инвертированный индекс** — в нем хранится идентификатор слова и перечисление документов, в которых оно содержится. Чтобы списки не были слишком большими, их обычно в оффлайне сортируют по полезности пр фичам, не зависящим от запроса (например, по PageRank) и рассматривают только первые несколько тысяч документов.

Очень сильно помогают индексированию новых документов логи из браузеров — поэтому почти все поисковые компанию делают либо полноценные браузеры, либо расширения типа яндексбара. Когда вы жмёте галочку «да, я хочу помочь Google / Microsoft / Yandex сделать мир лучше», содержимое посещенных вами страниц отправляется самим браузером напрямую поисковику. Летом 2018 года по этой причине приватные Google Docs начали появляться в выдаче Яндекса, а за несколько лет до этого внутренняя вики Яндекса попала в выдачу Google, после чего в компании запретили пользоваться Google Chrome.
