---
title: Линейная регрессия
---

Допустим, наша модель (зависимость) имеет вид
$$f(x_1, x_2, ... x_n) = a_0 + a_1x_1 + a_2x_2 + ... + a_nx_n$$

Мы хотим подобрать такие коэффициенты $a_0, a_1, ... a_n$, что какая-то
функция потерь на наших данных была минимальна. Для линейной регрессии в
качестве функции потерь используют MSE. Формально наша задача имеет вид:
$$\begin{array}{l}
\sum_{i=1}^{n}\left(f\left(x_{i 1}, x_{i 2}, x_{i 3}, ... x_{i n}\right)-y i\right)^{2} \rightarrow m i n \\
\sum_{i=1}^{n}\left(a_{0}+a_{1} x_{i 1}+a_{2} x_{i 2}+a_{3} x_{i 3}+ ... + x_{in}-\right.
\left.y_{i}\right)^{2} \rightarrow m i n
\end{array}$$

Если решать эту задачу, посчитав производные, то мы получим систему из n
линейных уравнений, решение которой и определяет коэффициенты
$a_0, a_1,... a_n$, при которых функция потерь минимальна.

У непрерывно-дифференцируемой функции, которая при стремлении по каждой
координате к плюс или минус бесконечности сама стремится к плюс
бесконечности, всегда существует глобальный минимум. В точке глобального
минимума все производные как раз равны нулю. Следовательно, существует
всегда хотя бы одно решение, и мы его найдем.

-![](img/poly16.png)


## Регуляризация.

$b = (\beta_0,\beta_1,...\beta_n$) тут это то же самое, что и в
предыдущем пункте $(a_0, a_1,... a_n)$

**Регуляризация** - это искусственное занижение всех параметров моделей
($\beta_0,\beta_1,...$). Это помогает упростить модель, т.к. низкие
значения $\beta_i$ означают, что модель будет выдавать результаты
близкие к прямой (или гиперплоскости, если параметров много). А также
зануление некоторых коэффициентов может убрать ненужные признаки из
обучения. Однако как всё это сделать так, чтобы не убрать нужные
признаки?

Посмотрим на функцию потерь, которую мы минимизируем:
$$L(\beta)=\sum_{i=1}^{N}(y^{true}_i-y^{pred}_i)^2$$

Добавим туда слагаемое (это L2-регуляризация), которое поможет нам
понизить значения наших $\beta_i$:
$$L(\beta)=\sum_{i=1}^{N}(y^{true}_i-y^{pred}_i)^2 + \lambda \sum_{i=1}^m\beta_i^2$$

Действительно, если мы будем понижать нашу функцию потерь $L(\beta)$, то
мы будем минимизировать и второе слагаемое, которое и отвечает за
абсолютные величины $\beta_i$. Осталось указать, что $\lambda$ это
некоторый числовой коэффициент, который позволяет играться между очень
сильной и очень слабой регуляризацией. Если он большой, то регуляризация
сильная и модель будет более простой, меньше переобучаться, больше
недообучаться, и наоброт.

L1-регуляризация выглядит так:
$$L(\beta)=\sum_{i=1}^{N}(y^{true}_i-y^{pred}_i)^2 + \lambda \sum_{i=1}^m|\beta_i|$$
Она отличается от L2 тем, что она обнуляет некоторые коэффициенты, а L2
пытается всё уменьшать равномерно.

В sklearn L1-регуляризация - это Lasso, а L2-регуляризация - это Ridge.
