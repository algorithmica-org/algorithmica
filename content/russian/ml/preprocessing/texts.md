---
title: Работа с текстами
---


Токенизация, Лемматизация, CountVectorizer, OneHotVectorizer, Стемминг.
Word2vec (идея, какие полезные свойства эмбеддингов даёт)

## Стоп-слова

Хотим уменьшить словарь. Плохие слова:

-   Слишком частые

-   русский язык: и, но, я, ты, \...

-   английский язык: a, the, I, \...

-   специфичные для коллекции: \"сообщать\" в новостях

-   Слишком редкие

-   Предлоги, междометия, частицы, цифры

## Токенизация

**Токенизация** -- это разделение текста на токены, элементарные
единицы.\
В ноутбуках были примеры токенизации просто разбиением текста по
пробелам, по регулярке, чтобы объединять целиком слова или имена с
фамилиями, которые считаются одним словом, или по регулярке, чтобы
токеном было целое предложение.

## Стемминг

**Стемминг** - нормализация слов путем отбрасывания окончаний (согласно
правилам, основанным на грамматике языка).\
Стеммеры(nltk):

-   Porter stemmer

-   Snowball stemmer

-   Lancaster stemmer

-   MyStem

В целом :

-   Плохо работает для русского языка

-   Нормально работает для английского

-   Повышает качество модели

## Лемматизация

**Лемматизация** - приведение слов к начальной морфологической форме (с
помощью словаря и грамматики языка).\
Лемматизаторы:

-   pymorphy2 (язык русский, украинский)

-   mystem3 (язык русский)

-   Wordnet Lemmatizer (NLTK, язык английский, требует POS метку)

-   Metaphraz (язык русский)

-   Coda/Cadenza (языки русский и английский)

Лемматизатор на самом деле довольно сложно устроены, им нужны теги
частей речи (POS).

По умолчанию функция WordNetLemmatizer.lemmatize () будет считать, что
это слово является существительным, если на входе не обнаружен тег POS.

Сначала вам понадобится функция pos_tag, чтобы пометить предложение и
использовать тег, чтобы преобразовать его в теги WordNet, а затем
передать его в WordNetLemmatizer.

Примечание. Лемматизация не будет работать только на одиночных словах
без контекста или знании своего тега POS

В целом:

-   Лучше стемминга для русского языка

-   Хорошо работает для английского языка

-   Повышает качество модели

-   Гораздо медленнее чем стемминг

## One-hot encoding

Представление словаря в виде бинарных векторов, у которых все значения
равны 0, кроме одного, отвечающего за соответствующее слово

## CountVectorizer

Представление словаря в виде векторов, у которых $i$-ая координата --
количество вхождений $i$-ого слова в предложение соотвествующее этому
вектору.

Может возникнуть проблема, что какое-то незначительное для классификации
предложения слово может встречаться очень часто и весить очень много,
хотя особого смысла в себе не несёт.

## TF-IDF Vectorizer

Для того чтобы решить эту проблему есть TF-IDF Vectorizer.

$$tf(t, d) = \frac{n_{t}}{\sum_{k} n_{k}}$$ Где $n_{t}$ -- число
вхождений слова $t$ в документ $d$, а в знаменателе -- общее число слов
в документе.
$$idf(t, D) = \log{\frac{|D|}{|\{d_{i} \in D | t \in d_{i}\}|}}$$ Где
$|D|$ -- число документов в коллекции, $|\{d_{i} \in D | t \in d_{i}\}|$
-- число документов из коллекции $D$, в которых встречается $t$(когда
$n_{t} \neq 0$)

$$tfidf(t, d, D) = tf(t, d) \times idf(t, D)$$

Большой вес в TF-IDF получат слова с высокой частотой в пределах
конкретного документа и с низкой частотой употреблений в других
документах.

## Word2Vec

Word2Vec - это нейросетевая модель.

Результатом работы данной модели является словарь эмбеддингов,
позволяющий сопоставлять словам их векторные представления.

Используя такое представление, можно по какому-то новому слову, найти
несколько ближайших к нему слов(имеется в виду несколько ближайших
вектороных представлений других слов).

Плюс такого подхода в том, что в нем похожим по значению словам
соответствуют похожие вектора - т.е. близкие друг к другу точки в
$n$-мерном пространстве.

У эмбеддингов, получаемых с помощью Word2Vec есть еще одна интересная
особенность - если мы можем сказать, что 'А относится к B, как С к D',
то вектора слов 'A - B' и 'C - D' будут довольно похожи. Это значит, что
если мы рассмотрим вектор 'A - B + D', то вектор 'C' часто будет
оказываться среди его ближайших соседей:

$'Paris' - 'France' + 'Germany' = 'Berlin$,

$'king' - 'man' + 'woman' = 'queen`$

Как мы видели выше, модели Word2Vec дают нам хороший способ представлять
слова в виде векторов. Но для решения задач ML нам нужно научиться
представлять в виде векторов тексты, а не отдельные слова.

Построенные на основе Word2Vec эмбеддинги текстов будут сохранять их
смысл и при этом их размерность будет гораздо меньше, чем у векторных
представлений, получаемых с помощью tf-idf. Такие эмбеддинги бывают
очень полезны в разных задачах ML.

Попробуем считать вектор документа, как среднее векторов слов из этого
документа.

Оказывается, что усреднять слова в документе - не лучший способ получить
его векторное представление. Есть много слов, которые встречаются часто
и есть почти во всех документах. При таком способе подсчета они будут
делать вектора документов слишком похожими друг на друга.

Значит, нам нужно давать словам разные веса, когда мы вычисляем вектор
документа. Большой вес должен быть у слов, которые часто встречаются в
этом документе, но редко в других.

Но это как раз **tf-idf**. (Также можно использовать просто idf. TF -
частота слова в документе - итак будет учтена, т.к. мы суммируем вектора
слов в документе).

Осталось понять, как посчитать такую взвешенную сумму векторов. Для
этого можно составить матрицу из векторов слов, встречающихся в нашей
коллекции (они обязательно должны идти в том же порядке, что и в матрице
из 'TfidfVectorizer' - но там слова будут столбцами, а тут - строками),
после чего перемножить 2 матрицы.

Пусть $DOCS$ -- число документов в коллекции, $WORDS$ -- число
уникальных слов, $DIM$ -- размерность пространства эмбеддингов из
Word2Vec.

Тогда первая матрица будет размера $(DOCS, WORDS)$,

вторая - $(WORDS, DIM)$,

а их произведение - $(DOCS, DIM)$ - это и будет матрица векторных
представлений для каждого документа в коллекции. Размерность матрицы
понижена, без использования $PCA$ и других алгоритмов.

