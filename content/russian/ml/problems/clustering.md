---
title: Кластеризация
---

В обучающих выборках, рассмотренных выше задач, нам была известна
целевая переменная:

-   Выплатит ли человек кредит вовремя --- в задаче классификации

-   Рост ребенка --- в задаче регрессии.

Однако, целевая переменная не всегда известна. Например, мы провели
социологический опрос, у нас есть много ответов на вопросы и нам
хотелось бы сгруппировать их по поведению. Мы заранее не знаем, сколько
таких кластеров получится и что они из себя представляют. Это мы можем
узнать только после того как появятся сами кластеры.

Еще простой и пробирочной задачей кластеризации является кластеризация
точек на плоскости

![](img/clustering.png)


Данная задача состоит в разделении точек в пространстве на кластеры
близко расположенных.

## Метрика качества кластеризации

Как и для задач обучения с учителем, хочется уметь понимать, хорошо ли
мы решили задачу - нужно научиться измерять качество кластеризации.

Так, например, даже не зная число кластеров, можно перебрать *К* в
**K-Means** и выбрать гиперпараметр с лучшим качеством.

Одним из примеров метрики может быть метрика **силуэт**.

Для одного элемента $x$ она считается так:

$S(x)=\frac{b(x)-a(x)}{\max (a(x), b(x))}$

$a(x)=$ среднее расстояние от $x$ до точек того же кластера.

$b(x)=$ среднее расстояние от $x$ до точек ближайшего кластера.

Значение *силуэта* для кластера равно среднему значению $S(x)$ от
каждого элемента.

Чем он больше тем лучше кластер отделим от других кластеров.

Примеры из выборки можно отсортировать по значению *силуэта* для каждого
примера и по присвоенной метке кластера. Такое представление позволяет
отфильтровать шумные примеры (у которых силуэт меньше некоторого
порога).

## Алгоритм K-Means

Нужно в начале выбрать K центров кластеров (K - гиперпараметр). Далее
итеративно выполняются 2 шага, пока обновления не перестанут
происходить:

1.  Обновить кластеры, приписав каждой точке кластер самого близкого к
    ней центра

2.  Обновить центр каждого кластера как центр масс его точек

![](img/k-means.png)

Плюсы:

-   простой и понятный

Минусы :

-   нужно знать K

-   слишком простая модель, кластер = выпуклая околокруглая штука, так
    как это по сути диаграмма Вороного

-   если плохо выбрать начальные центры, может сойтись к плохому
    результату

Поэтому обычно K-Means запускают несколько раз и выбирают лучший
результат.

## DBSCAN

**DBSCAN** строит столько кластеров, сколько получится, причем многие
вершины могут не войти ни в один кластер, они называются выбросами.

*DBSCAN* опирается на два гиперпараметра:

*eps* - означает расстояние, на котором две вершины считаются соседями.

*min-samples* - означает сколько нужно соседей из кластера, чтобы
считать вершину коренной вершиной кластера.

Сам алгоритм состоит из таких шагов:

1.  Выбрать соседей для каждой вершины на расстоянии до eps

2.  Найти компоненты связности коренных вершин - добавляем вершину в
    компоненту коренных, если у нее хотя бы min-samples соседей лежат в
    этой компоненте

3.  Добавить оставшиеся вершины в самый популярный кластер соседей, если
    есть соседи

4.  Оставшиеся вершины - это выбросы

![](img/dbscan.png)

Плюсы :

-   сам подберет число кластеров

-   опирается на плотность точек, кластеры могут быть вытянутыми и даже
    невыпуклыми

Минусы :

-   нужно подбирать два других параметра

-   алгоритм считает, что в разных частях данных плотности должны быть
    примерно одинаковыми

## Агломеративная кластеризация

Интуиция у алгоритма очень простая:

1.  Начинаем с того, что высыпаем на каждую точку свой кластер

2.  Сортируем попарные расстояния между центрами кластеров по
    возрастанию

3.  Берём пару ближайших кластеров, склеиваем их в один и пересчитываем
    центр кластера

4.  Повторяем п. 2 и 3 до тех пор, пока все данные не склеятся в один
    кластер

Чтобы найти пару ближайших кластеров берут не только расстояние между
центрами, бывают и такие метрики:

-   Single linkage --- минимум попарных расстояний между точками из двух
    кластеров

-   Complete linkage --- максимум попарных расстояний между точками из
    двух кластеров

-   Average linkage --- среднее попарных расстояний между точками из
    двух кластеров

-   Centroid linkage --- расстояние между центроидами двух кластеров

![](img/agglomerative.jpg)

По итогам выполнения такого алгоритма строится дерево склеивания
кластеров. Глядя на него можно определить, на каком этапе оптимальнее
всего остановить алгоритм.
