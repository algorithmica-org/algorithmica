---
title: SVD и PCA
---

Уменьшение размерности и визуализация.

Обе эти задачи состоят в сжатии выборки: нужно перевести точки из
*N-мерного* пространства в *M-мерное* пространство, где $M < N$, причем
так, чтобы близкие точки остались близкими. То есть хочется значительно
уменьшить число признаков, не сильно потеряв (или даже улучшив) их
качество.

## SVD-разложение

Пусть $A$ - прямоугольная матрица признаков. Тогда она представима
единственным образом в следующем виде: $$A = U \Sigma V$$

где:

$\Sigma$ -- прямоугольная диагональная матрица с неотрицательными
убывающими элементами на диагонали,

$U, V$ -- квадратные унитарные матрицы

Cмысл такой, что преобразование пространства под действием оператора с
унитарной матрицей -- сохраняет скалярное произведение, то есть грубо
говоря это поворот пространства.

Существует теорема Эккарта-Янга, которая утверждает, что можно
приблизить матрицу $A$ матрицей меньшего ранга $k < n$, причем наилучшая
матрица(в плане минимальной нормы разницы матриц) -- будет матрица
полученная усечённым **SVD-разложением**.

### Усечённое SVD-разложение {#усечённое-svd-разложение .unnumbered}

Выберем $k$ -- ранг матрицы, которой будем приближать исходную матрицу
$A$, с практической точки зрения это количество признаков, которые мы
хотим оставить.

Применим SVD-разложение.

$$A = U \Sigma V$$

Оставим только $k$ столбцов в $U$ и $k$ строк в $V$, а так же занулим
все, кроме $k$ первых чисел на диагонали в $\Sigma$, обрежем ее до
размерности $k \cdot k$.

Назовём это $A_{k} = U_{k} \Sigma_{k} V_{k}$.

Такое приближение матрицы $A$ матрицей ранга $k$ -- оптимально, а
матрица $U_{k} \Sigma_{k}$ -- называется **усечённым SVD-разложением**
матрицы $A$ и содержит ровно $k$ столбцов, то есть уменьшилось
количество признаков. В sklearn этот метод лежит в
sklearn.decomposition.TruncatedSVD.

## PCA - метод главных компонент

Метод **PCA** заключается в том, чтобы найти в N-мерном пространстве
такое K-мерное пространство, что проекция всех точек на него будет как
можно более рассеянной (то есть иметь наибольшую дисперсию).

Оказывается, подходит пространство, сумма квадратов расстояния от
которого до всех точек минимальна.

Первая компонента (ось) выбирается так, чтобы дисперсия проекции вдоль
нее была максимальна.

После этого все точки проецируются вдоль первой компоненты и там таким
же способом выбирается вторая компонента и так далее.

Получившиеся $k$ компонент будут образовывать K-мерное пространство,
проекция точек на которое будет иметь наибольшую дисперсию.

Оказывается, если применить *TruncatedSVD* к центрированным данным
(среднее по каждой координате равно 0), то именно это и получится!

В каком-то смысле PCA - это лучший линейный способ уменьшить размерность
пространства.

![](../img/pca.png)

![](../img/pca-sample.png)

### Плюсы PCA {#плюсы-pca .unnumbered}

-   считается относительно быстро

-   детерминированность (зависит от реализации, в sklearn не так)

-   любая размерность

-   полученную модель уменьшения размерности можно применять и к новым
    точкам (нужно просто спроецировать ее на выделенное $k$-мерное
    пространство)

-   визуализация при $k = 2$ - это честная проекция на какую-то
    плоскость, легкая интерпретируемость результатов

-   при увеличении размерности старые координаты не меняются

-   отлично подходит для отбора признаков

### Минусы PCA {#минусы-pca .unnumbered}

-   алгоритм слишком простой - он линейный

-   некоторые непохожие точки могут спроецироваться в одну и ту же при
    $k = 2$

